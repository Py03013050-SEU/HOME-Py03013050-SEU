{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22 LIES, DAMNED LIES, AND STATISTICS\n",
    "\n",
    "\n",
    "**There are lies, damned lies and statistics.** - Mark Twain \n",
    "\n",
    "---\n",
    "**Statistical thinking** is a relatively new invention.\n",
    "\n",
    "This started to change in the middle of the 17th century, most notably with the publication of\n",
    "**John Graunt** ’s \n",
    "\n",
    "* **Natural and Political Observations Made Upon the Bills of Mortality**\n",
    "\n",
    "Since that time people have used statistics as much to **mislead as to inform**.\n",
    "\n",
    "* Some have  **willfully** used statistics to mislead;\n",
    "\n",
    "* others have merely been **incompetent**\n",
    "\n",
    "We trust that you will use this information only for good,\n",
    "\n",
    "  * **a better consumer** \n",
    "  \n",
    "  * **a more honest purveyor of statistical information**.\n",
    "\n",
    "1 Garbage In Garbage Out (GIGO)\n",
    "\n",
    "2 Tests Are Imperfect\n",
    "\n",
    "3 Pictures Can Be Deceiving\n",
    "\n",
    "4 Cum Hoc Ergo Propter Hoc\n",
    "\n",
    "5 Statistical Measures Don’t Tell the Whole Story\n",
    "\n",
    "6 Sampling Bias\n",
    "\n",
    "7 Context Matters\n",
    "\n",
    "8 Beware of Extrapolation\n",
    "\n",
    "9 The Texas Sharpshooter Fallacy\n",
    "\n",
    "10 Percentages Can Confuse\n",
    "\n",
    "11 Statistically Significant Differences Can Be Insignificant\n",
    "\n",
    "12 The Regressive Fallacy\n",
    "\n",
    "13 Just Beware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.1 Garbage In Garbage Out (GIGO)\n",
    "\n",
    "If the input data is seriously flawed, no amount of statistical massaging will produce a meaningful result.\n",
    "\n",
    "Calhoun’s (perhaps willfully) spurious response to Adams was based on a classical error, \n",
    "\n",
    "**the assumption of independence**\n",
    "\n",
    "### Assumption of Independence\n",
    "\n",
    "The assumption of independence is used for several other statistical tests.\n",
    "\n",
    "It’s essential to getting results from your <b>sample</b> that reflect what you would find in a <b>population</b>. \n",
    "\n",
    "Even the smallest dependence in your data can turn into heavily biased results (which may be undetectable) if you violate this assumption.\n",
    "\n",
    "A <b>dependence</b> is <b>a connection</b> between your data. For example, how much you earn depends upon how many hours you work. \n",
    "\n",
    "<b>Independence</b> means there <b>isn’t a connection</b>. For example, how much you earn isn’t connected to what you ate for breakfast. \n",
    "\n",
    "The <b>assumption of independence</b> means that your <b>data isn’t connected in any way</b> (at least, in ways that you haven’t accounted for in your model).\n",
    "\n",
    "There are actually two assumptions: \n",
    "\n",
    "1 **The observations between groups should be independent,**\n",
    "\n",
    "which basically means the groups are made up of different people. You don’t want one person appearing twice in two different groups as it could skew your results.\n",
    "   \n",
    "\n",
    "2 **The observations within each group must be independent.** \n",
    "\n",
    "If two or more data points in one group are connected in some way, this could also skew your data. For example, let’s say you were taking a snapshot of how many donuts people ate, and you took snapshots every morning at 9,10, and 11 a.m.. You might conclude that office workers eat 25% of their daily calories from donuts. However, you made the mistake of timing the snapshots too closely together in the morning when people were more likely to bring bags of donuts in to share (making them dependent). If you had taken your measurements at 7, noon and 4 p.m., this would probably have made your measurements independent.\n",
    "\n",
    "#### How do I Avoid Violating the Assumption?\n",
    "\n",
    "Unfortunately, looking at your data and trying to see if you have independence or not is usually difficult or impossible. \n",
    "\n",
    "The key to avoiding violating the assumption of independence is to make sure your data is independent while you are collecting it. If you aren’t an expert in your field, this can be challenging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.2 Tests Are Imperfect\n",
    "\n",
    "**Every experiment should be viewed as a `potentially flawed` test**. We can perform a test for a chemical, a phenomenon, a disease, etc. However, the event for which we are testing is not necessarily the same as the result of the test.\n",
    "\n",
    "Professors design exams with the goal of understanding how well a student has mastered some subject matter, but the result of the exam should not be confused with how much a student actually understands. \n",
    "\n",
    "Every test has some inherent error rate. Imagine that a student learning a second language has been asked to learn the meaning of 100 words, but has learned the meaning of only 80 of them. His rate of understanding is 80%, but the probability that he will score 80% on a test with 20 words is certainly not 1.\n",
    "\n",
    "**Tests can have both false negatives and false positives**. As we saw in Chapter 20, a negative mammogram does not guarantee absence of breast cancer, and a positive mammogram doesn’t guarantee its presence. Furthermore, the test probability and the event probability are not the same thing. This is especially relevant when testing for a rare event, e.g., the presence of a rare disease. If the cost of a false  negative is high(e.g., missing the presence of a serious but curable disease), the test should be designed to be highly sensitive, even at the cost of there being a large number of false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.3 Pictures Can Be Deceiving\n",
    "\n",
    "There can be no doubt about the utility of graphics for quickly conveying information. However, when used carelessly (or maliciously) a plot can be highly misleading.\n",
    "\n",
    "Consider, for example, the following charts depicting housing prices in the U.S. Midwestern states.\n",
    "<img src=\"./img/162.PNG\"/> \n",
    "\n",
    "Looking at the chart on the left, it seems as if housing prices were <b>pretty stable</b> from 2006-2009.\n",
    "\n",
    "But wait a minute, wasn’t there <b>a collapse of U.S. residential real estate</b> followed by a global financial crisis in late 2008? There was indeed, as shown in <b>the chart on the right</b>.\n",
    "\n",
    "The first chart was <b>designed to give the impression</b> that housing prices had been <b>stable</b>.\n",
    "\n",
    "On the y-axis, the designer used <b>a logarithmic scale</b> ranging from the absurdly low average price for a house of <b>10,000</b> to the improbably high average price of <b>1 million</b>. This <b>minimized the amount of space</b> devoted to the area where prices are changing, giving the impression that the changes were relatively small.\n",
    "\n",
    "The chart above and on the right was <b>designed to give the impression</b> that housing prices <b>moved erratically, and then crashed</b>.\n",
    "\n",
    "The designer used <b>a linear scale and a narrow range of prices</b>, so the sizes of the changes were <b>exaggerated</b>.\n",
    "\n",
    "The next code produces the two plots we looked at above and <b>a plot intended to give an accurate impression of the movement of housing prices</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./data/midWestHousingPrices.txt\n",
    "2006 01 210700\n",
    "2006 02 203100\n",
    "2006 03 216800\n",
    "2006 04 216200\n",
    "2007 01  212800\n",
    "2007 02  203200\n",
    "2007 03  209600\n",
    "2007 04  197400\n",
    "2008 01 219200\n",
    "2008 02 198500\n",
    "2008 03 184700\n",
    "2008 04 202500\n",
    "2009 01  187100\n",
    "2009 02  193200\n",
    "2009 03  184900\n",
    "2009 04  196000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "def midWestHousingPrices():\n",
    "    f = open('./data/midWestHousingPrices.txt', 'r')\n",
    "    \n",
    "    #Each line of file contains year quarter price\n",
    "    #for Midwest region of U.S.\n",
    "    \n",
    "    labels, prices = ([], [])\n",
    "    for line in f:\n",
    "        year, quarter, price = line.split()\n",
    "        \n",
    "        label = year[2:4] + '\\n Q' + quarter[1]\n",
    "        labels.append(label)\n",
    "        prices.append(float(price)/1000)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return labels, prices\n",
    "\n",
    "def plotHousing(impression):\n",
    "    \"\"\"Assumes impression a str.  Must be one of \n",
    "         'flat','volatile,' and 'fair'\n",
    "       Produce bar chart of housing prices over time\"\"\"\n",
    "    quarters =np.arange(len(labels)) # x coords of bars\n",
    "    width = 0.8 #Width of bars\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    if impression == 'flat':\n",
    "        plt.semilogy()\n",
    "        \n",
    "    plt.bar(quarters, prices, width)\n",
    "    plt.xticks(quarters+width/2.0, labels)\n",
    "    plt.title('Housing Prices in U.S. Midwest')\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average Price ($1,000\\'s)')\n",
    "    \n",
    "    if impression == 'flat':\n",
    "        plt.ylim(10, 10**3)\n",
    "    elif impression == 'volatile':\n",
    "        plt.ylim(180, 220)\n",
    "    elif impression == 'fair':\n",
    "        plt.ylim(150, 250)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "labels, prices = ([], [])\n",
    "labels, prices=midWestHousingPrices()\n",
    "\n",
    "plotHousing('flat')\n",
    "plotHousing('volatile')\n",
    "plotHousing('fair')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses two plotting facilities that we have not yet seen. \n",
    "\n",
    "* **plt.bar**\n",
    "\n",
    "* **plt.xticks**\n",
    "\n",
    "```python\n",
    "plt.bar(quarters, prices, width) \n",
    "```\n",
    "produces a bar chart with `width` wide bars. The left edges of the bars are the values of the elements of quarters and\n",
    "the heights of the bars are the values of the corresponding elements of prices.\n",
    "\n",
    "```python\n",
    "plt.xticks(quarters+width/2.0, labels) \n",
    "```\n",
    "describes the labels associated with the bars. \n",
    "\n",
    " * The first argument: `quarters+width/2.0`:  specifies where each label is to be placed \n",
    "\n",
    " * The second argument:`labels` : the text of the labels. \n",
    "\n",
    "The function `yticks` behaves analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.4 Cum Hoc Ergo Propter Hoc\n",
    "\n",
    "Statisticians, like attorneys and physicians, sometimes use Latin for no obvious reason\n",
    "other than to seem erudite. This phrase means, “with this, therefore because of this.”\n",
    "\n",
    "When two things are <b>correlated</b>, there is a temptation to assume that one has <b>caused</b> the other.\n",
    "\n",
    "#### But Correlation does not imply causation!\n",
    "\n",
    "there is some <b>lurking variable</b> that we have not considered that causes each.\n",
    "\n",
    "Given enough retrospective data, it is <b>always possible to find two variables that are correlated </b>\n",
    "<img src=\"./img/163.PNG\"/>\n",
    "\n",
    "When such correlations are found, the first thing to do is to <b>ask whether there is a plausible theory explaining the\n",
    "correlation</b>.\n",
    "\n",
    "Falling prey to the `cum hoc ergo propter hoc` fallacy can be <b>quite dangerous</b>.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.5 Statistical Measures Don’t Tell the Whole Story\n",
    "\n",
    "There are an enormous number of different statistics that can be extracted from a data set.\n",
    "\n",
    "By carefully choosing among these, it is possible to convey <b>a variety of different impressions about the same data</b>. \n",
    "\n",
    "A good **antidote** is to **look at the data set itself**.\n",
    "\n",
    "### 22.5.1 F.J. Anscombe's Data Sets\n",
    "\n",
    "In 1973, the statistician F.J. Anscombe published a paper containing the table below. It contains the <x, y> coordinates of the points in each of four data sets. \n",
    "\n",
    "Each of the four data sets has \n",
    "\n",
    "* the same mean value for x (9.0), the same mean value for y (7.5), \n",
    "\n",
    "* the same variance for x (10.0), the same variance for y (3.75),\n",
    "\n",
    "* the same correlation between x and y (0.816)\n",
    "\n",
    "* if we use linear regression to fit a line to each, we get the same result for each, y = 0.5x + 3.\n",
    "\n",
    "|x0 |y0|x1|y1|x2|y2|x3|y3|\n",
    "|----:|------:|------:|------:|------:|------:|------:|----:|\n",
    "|10.0|\t8.04 |\t10.0|\t9.14 |\t10.0 \t|7.46\t|8.0    |6.58|\n",
    "|8.0| \t6.95 |\t8.0 |\t8.14 |\t8.0     |6.77\t|8.0    |5.76|\n",
    "|13.0| \t7.58 |\t13.0|\t8.74 |\t13.0    |12.74\t|8.0    |7.71|\n",
    "|9.0|\t8.81 |\t9.0 |\t8.77 |\t9.0     |7.11\t|8.0    |8.84|\n",
    "|11.0| \t8.33 |\t11.0|\t9.26|\t11.0    |7.81\t|8.0    |8.47|\n",
    "|14.0| \t9.96 |\t14.0|\t8.10 |\t14.0    |8.84\t|8.0    |7.04|\n",
    "|6.0|\t7.24 |\t6.0 |\t6.13 |\t6.0     |6.08\t|8.0 \t|5.25|\n",
    "|4.0| \t4.26 |\t4.0 |\t3.10| \t4.0     |5.39\t|19.0 \t|12.5|\n",
    "|12.0|\t10.84| \t12.0| \t9.13| \t12.0    |8.15\t|8.0 \t|5.56|\n",
    "|7.0| \t4.82 | \t7.0 |\t7.26| \t7.0     |6.42\t|8.0 \t|7.91|\n",
    "|5.0| \t5.68 | \t5.0 | \t4.74| \t5.0     |5.73\t|8.0 \t|6.89|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this mean that there is no obvious way to distinguish these data sets from\n",
    "each other? No, one simply needs to plot the data to see that the data sets are\n",
    "not at all alike.\n",
    "\n",
    "![164curve](./img/164curve.PNG)\n",
    "\n",
    "\n",
    "<b style=\"font-size:120%;color:bluer\"> The moral is simple: if possible, always take a look at some representation of the raw data.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use `statistics,scipy` for Statistical Measures\n",
    "\n",
    "`statistics.mean(data)`\n",
    "\n",
    "* Return the sample arithmetic mean of data which can be a sequence or iterator.\n",
    "\n",
    "`statistics.pvariance(data, mu=None)`\n",
    "\n",
    "* Return the population variance of data, a non-empty iterable of real-valued numbers\n",
    "\n",
    "**Pearson correlation**\n",
    "\n",
    "$r=\\frac{\\sum_{i=1}^n (X_i- \\overline X)(Y_i- \\overline Y)}{ \\sqrt{ \\sum_{i=1}^n (X_i- \\overline X)^2} \\cdot \\sqrt{ \\sum_{i=1}^n (Y_i- \\overline Y)^2}  }$\n",
    "\n",
    "`scipy.stats.pearsonr(x, y)`\n",
    "\n",
    "* Calculate a Pearson correlation coefficient and the p-value for testing non-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  statistics import mean,pvariance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "x0=[10.0, 8.0,13.0, 9.0,11.0,\n",
    "    14.0, 6.0, 4.0,12.0, 7.0,5.0]\n",
    "y0=[8.04,6.95,7.58,8.81,8.33,\n",
    "    9.96,7.24,4.26,10.84,4.82,5.68]\n",
    "print('statistics.mean:')\n",
    "print('\\t X0:',mean(x0))\n",
    "print('\\t Y0:',mean(y0))\n",
    "print('statistics.pvariance:')\n",
    "print('\\t X0:',pvariance(x0))\n",
    "print('\\t Y0:',pvariance(y0))\n",
    "print('corrcoef:')\n",
    "print('\\t stats.pearsonr',pearsonr(x0,y0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.5.2 Analyzing with Numpy and Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./data/anscombe.csv\n",
    "x0,y0,x1,y1,x2,y2,x3,y3\n",
    "10.00 ,8.04 ,10.00 ,9.14 ,10.00 ,7.46 ,8.00 ,6.58 \n",
    "8.00 ,6.95 ,8.00 ,8.14 ,8.00 ,6.77 ,8.00 ,5.76 \n",
    "13.00 ,7.58 ,13.00 ,8.74 ,13.00 ,12.74 ,8.00 ,7.71 \n",
    "9.00 ,8.81 ,9.00 ,8.77 ,9.00 ,7.11 ,8.00 ,8.84 \n",
    "11.00 ,8.33 ,11.00 ,9.26 ,11.00 ,7.81 ,8.00 ,8.47 \n",
    "14.00 ,9.96 ,14.00 ,8.10 ,14.00 ,8.84 ,8.00 ,7.04 \n",
    "6.00 ,7.24 ,6.00 ,6.13 ,6.00 ,6.08 ,8.00 ,5.25 \n",
    "4.00 ,4.26 ,4.00 ,3.10 ,4.00 ,5.39 ,19.00 ,12.50 \n",
    "12.00 ,10.84 ,12.00 ,9.13 ,12.00 ,8.15 ,8.00 ,5.56 \n",
    "7.00 ,4.82 ,7.00 ,7.26 ,7.00 ,6.42 ,8.00 ,7.91 \n",
    "5.00 ,5.68 ,5.00 ,4.74 ,5.00 ,5.73 ,8.00 ,6.89 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Load data from a text file\n",
    "\n",
    "**[numpy.genfromtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html#numpy.genfromtxt)** ：Load data from a text file, with missing values handled as specified. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vec = np.genfromtxt('./data/anscombe.csv', skip_header=1, delimiter=',', dtype=None)\n",
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**np.array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row 2+1,colume 3+1\n",
    "vec[2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all row,column 1\n",
    "vec[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Statistical Measures\n",
    "\n",
    "[numpy.mean](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html)： Compute the arithmetic mean along the specified axis.\n",
    "\n",
    "[numpy.var](https://docs.scipy.org/doc/numpy/reference/generated/numpy.var.html)： Compute the variance along the specified axis.\n",
    "\n",
    "[numpy.corrcoef](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html)：Return Pearson product-moment correlation coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupcount=vec.shape[1]//2\n",
    "# rows:4 group, columns: 2 (x,y)\n",
    "means = np.zeros(shape=(groupcount,2))\n",
    "pvars = np.zeros(shape=(groupcount,2))\n",
    "corrcoefs = np.zeros(groupcount)\n",
    "for i in range(groupcount):\n",
    "    means[i]=[np.mean(vec[:,i*2]),np.mean(vec[:,1+i*2])]\n",
    "    pvars[i]=[np.var(vec[:,i*2]),np.var(vec[:,1+i*2])]\n",
    "    corrcoefs[i]=np.corrcoef(vec[:,i*2],vec[:,1+i*2])[0,1]\n",
    "print(means)  \n",
    "print(pvars)\n",
    "print(corrcoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3 Linear Regression\n",
    "\n",
    "**[numpy.polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)**\n",
    "\n",
    "* Least squares polynomial fit.Fit a polynomial $p(x) = p[0] * x^{deg} + ... + p[deg]$ of degree deg to points $(x, y)$. Returns a vector of coefficients $p$ that minimises the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(groupcount)\n",
    "b= np.zeros(groupcount)\n",
    "for i in range(groupcount):\n",
    "    a[i],b[i] = np.polyfit(vec[:,i*2],vec[:,1+i*2], 1)\n",
    "    print(a[i],b[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Plotting：Subplot\n",
    "\n",
    "[matplotlib.pyplot.figure.subplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html): \n",
    "\n",
    "Add a **subplot** to the current figure\n",
    "\n",
    "```python\n",
    "subplot(nrows, ncols, index)\n",
    "```\n",
    "[matplotlib.pyplot.subplots_adjust](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots_adjust.html): \n",
    "\n",
    "Tune the **subplot** layout\n",
    "\n",
    "```python\n",
    "subplots_adjust(left=None, bottom=None, right=None, top=None)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotData(x,y,i,ai,bi,fx,fy):\n",
    "    plt.title('No: '+str(i+1))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    # data points\n",
    "    plt.plot(x,y, 'bo')\n",
    "    \n",
    "    # the linear fit line  \n",
    "    streq=f\"$y={ai:.5f}x+{bi:.5f}$\"\n",
    "    _label= \"Linear Regression:$y=f(x)$\"+\"\\n\"+streq\n",
    "    plt.plot(fx, fy, 'k-',label=_label)\n",
    "    \n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "figcol=2\n",
    "figrow=math.ceil(groupcount/figcol)\n",
    "\n",
    "fig=plt.figure(figsize=(12.0,8.0))\n",
    "\n",
    "fig.subplots_adjust(left=0.05,right=0.95,bottom=0.05,top=0.95)\n",
    "\n",
    "for i in range(groupcount):\n",
    "    #  plt.subplot\n",
    "    plt.subplot(figrow, figcol,i+1)\n",
    "    plt.xlim(0 ,20)\n",
    "    plt.ylim(2 ,14)\n",
    "    \n",
    "    fx=np.array([0,20])\n",
    "    fy=a[i]*fx+b[i]\n",
    "    \n",
    "    plotData(vec[:,i*2],vec[:,1+i*2],i,a[i],b[i],fx,fy)\n",
    "\n",
    "plt.savefig(\"./img/Anscombe.svg\")\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.6 Sampling Bias\n",
    "\n",
    "####  non-response bia\n",
    "\n",
    "During World War II, whenever an Allied plane would return from a mission over Europe the plane would be inspected to see where the flak had impacted. Based upon this data, mechanics reinforced those areas of the planes that seemed most likely to be hit by flak.\n",
    "\n",
    "What’s wrong with this? They did not inspect the planes that failed to return from missions because they had been downed by flak. Perhaps these unexamined planes failed to return precisely because they were hit in the places\n",
    "where the flak would do the most damage. \n",
    "\n",
    "This particular error is called <b>non-response bias</b>.\n",
    "\n",
    "#### sampling a subset of a population we can infer things about the population as a whole?\n",
    "\n",
    "All statistical techniques are based upon the assumption that by sampling a subset of a population we can infer things about the population as a whole.\n",
    "\n",
    "Unfortunately, many studies, particularly in the social sciences, are based on what has been called <b>convenience (or accidental) sampling</b>.\n",
    "\n",
    "\n",
    "A convenience sample <b>might be</b> representative, but there is no way of knowing <b>whether it actually</b> is\n",
    "representative.\n",
    "\n",
    "The Family Research Institute’s Web site contains a table with the following information:\n",
    "<img src=\"./img/165.PNG\"/>\n",
    "Pretty scary stuff if your sexual preference is other than heterosexual—until one  looks at how the data was compiled.\n",
    "\n",
    "How does one go about evaluating such a sample? \n",
    "\n",
    "One technique is to <b>compare data compiled from the sample against data compiled elsewhere</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.7 Context Matters\n",
    "\n",
    "It is easy to read more into the data than it actually implies, especially when viewing the data <b>out of context</b>.\n",
    "\n",
    "Opponents of government initiatives to reduce the prevalence of guns in the U.S. are fond of quoting the statistic that roughly <b>99.8%</b> of the firearms in the U.S. will not be used to commit a violent crime in any given year. \n",
    "\n",
    "Does this mean that there is <b>not much gun violence in the U.S</b>? \n",
    "\n",
    "**NO!**\n",
    "\n",
    "The National Rifle Association reports that that there are roughly 300 million privately owned firearms in the\n",
    "U.S.—<b>0.2%</b> of 300 million is <b>600,000</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.8 Beware of Extrapolation\n",
    "\n",
    "It is all too easy to extrapolate from data.\n",
    "\n",
    "Extrapolation should be done <b>only</b> when one has <b>a sound theoretical justification</b> for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.9 The Texas Sharpshooter Fallacy\n",
    "\n",
    "Imagine that you are driving down a country road in Texas. You see a barn that has six targets painted on it, and a bullet hole at the very center of each target.\n",
    "\n",
    "“Yes sir,” says the owner of the barn,“I never miss.” “That’s right,” says his spouse, “there ain’t a man in the state of Texas who’s more accurate with a paint brush.”\n",
    "\n",
    "Got it? <b>He fired the six shots, and then painted the targets around them</b>.\n",
    "\n",
    "A classic of the genre appeared in 2001.108 It reported that a research team at the Royal Cornhill hospital in Aberdeen had discovered that “anorexic women are most likely to have been born in the spring or early summer… Between March and June there were 13% more anorexics born than average, and 30% more in June itself.”\n",
    "\n",
    "#### Let’s look at that worrisome statistic for those women born in June.\n",
    "\n",
    "Let’s write a short program to see if we can reject the null hypothesis that this occurred purely by chance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def juneProb(numTrials):\n",
    "    june48 = 0\n",
    "    for trial in range(numTrials):\n",
    "        june = 0\n",
    "        for i in range(446):\n",
    "            if random.randint(1,12) == 6:\n",
    "                june += 1\n",
    "        if june >= 48:\n",
    "            june48 += 1\n",
    "    jProb = june48/float(numTrials)\n",
    "    print(f'Probability of at least 48 births in June ={jProb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juneProb(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as if the probability of at least 48 babies being born in June purely by chance is around 4.5%.\n",
    "\n",
    "Well, they might have been on to something had they started with the hypothesis that more babies who will become anorexic are born in June, and then run a study designed to check that hypothesis.\n",
    "\n",
    "#### But that is not what they did. \n",
    "Instead, they looked at the data and then, imitating the Texas sharpshooter, drew a circle around June.\n",
    "\n",
    "The right statistical question to have asked is what is <b>the probability</b> that there was <b>at least one month</b> (out of 12) in which at least 48 babies were born.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anyProb(numTrials):\n",
    "    anyMonth48 = 0\n",
    "    for trial in range(numTrials):\n",
    "        months = [0]*12\n",
    "        for i in range(446):\n",
    "            months[random.randint(0,11)] += 1\n",
    "        if max(months) >= 48:\n",
    "            anyMonth48 += 1\n",
    "    aProb = anyMonth48/float(numTrials)\n",
    "    print(f'Probability of at least 48 births in some month ={aProb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anyProb(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that it is not so unlikely after all that the results reported in the study reflect <b>a chance occurrence</b> rather <b>a real association between birth month and anorexia</b>.\n",
    "\n",
    "One doesn’t have to come from Texas to fall victim to the Texas Sharpshooter Fallacy.\n",
    "\n",
    "What next steps might the Aberdeen group have taken to test their newfound hypothesis?\n",
    "\n",
    "One possibility is to conduct <b>a prospective study</b>. In a prospective study, one starts with a set of hypotheses and then gathers data with the potential to either refute or confirm the hypothesis. If the group conducted a new study and got similar results, one might be convinced. Prospective studies can be expensive and time consuming to perform.\n",
    "\n",
    "Prospective studies can be expensive and time consuming to perform.\n",
    "\n",
    "In <b>a retrospective study</b>, one has to examine existing data in ways that reduce the likelihood of getting misleading results. One common technique, as discussed in Chapter 15, is to split the data into <b>a training set</b> and <b>a holdout set</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.10 Percentages Can Confuse\n",
    "\n",
    "When thinking about percentages, we always need to pay attention to the basis on which the percentage is computed.\n",
    "\n",
    "Percentages can be particularly misleading when applied to a small basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.11 Statistically Significant Differences Can Be Insignificant\n",
    "\n",
    "An admissions officer at the Maui Institute of Technology (MIT), wishing to convince\n",
    "the world that MIT’s admissions process is “gender-blind,” trumpeted, “At MIT, there\n",
    "is no significant difference between the grade point averages of men and women.” The\n",
    "same day, an ardent female chauvinist proclaimed that “At MIT, the women have a\n",
    "significantly higher grade point average than the men.” A puzzled reporter at the student\n",
    "newspaper decided to examine the data and expose the liar. But when she finally\n",
    "managed to pry the data out of the university, she concluded that both were telling the\n",
    "truth.\n",
    "\n",
    "What does the sentence, “At MIT, the women have a significantly higher grade point\n",
    "average than the men,” actually mean? People who have not studied statistics (most of\n",
    "the population) would probably conclude that there is a “meaningful” difference\n",
    "between the GPAs of women and men attending MIT. In contrast, those who have\n",
    "recently studied statistics might conclude only that 1) the average GPA of women is\n",
    "higher than that of men, and 2) the null hypothesis that the difference in GPA can be\n",
    "attributed to randomness can be rejected at the 5% level\n",
    "\n",
    "Suppose, for example, that there were 2500 women and 2500 men studying at MIT.\n",
    "Suppose further that the mean GPA of men was 3.5, the mean GPA of women was 3.51,\n",
    "and the standard deviation of the GPA for both men and women was 0.25. Most\n",
    "sensible people would consider the difference in GPAs “insignificant.” However, from\n",
    "a statistical point of view the difference is “significant” at close to the 2% level. What\n",
    "is the root of this strange dichotomy? As we showed in Section 19.5, when a study has\n",
    "enough power—i.e, enough examples—even insignificant differences can be\n",
    "statistically significant.\n",
    "\n",
    "\n",
    "A related problem arises when a study is very small. Suppose you flipped a coin\n",
    "twice and it came up heads both times. Now, let’s use the two-tailed one-sample t-test\n",
    "we saw in Section 19.3 to test the null hypothesis that the coin is fair. If we assume that\n",
    "the value of heads is 1 and the value of tails is 0, we can get the p-value using the code\n",
    "stats.ttest_1samp([1, 1], 0.5)[1]\n",
    "\n",
    "It returns a p-value of 0, indicating that if the coin is fair the probability of getting two\n",
    "consecutive heads is nil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.12 The Regressive Fallacy\n",
    "\n",
    "The regressive fallacy occurs when people fail to take into account the `natural fluctuations of events`.\n",
    "\n",
    "\n",
    "All athletes have good days and bad days. When they have good days, they try not to change anything. When they have a series of unusually bad days, however, they often try to make changes. Whether or not the changes are actually constructive, regression to the mean (Section 15.3) makes it likely that over the next few days the athlete’s performance will be better than the unusually poor performances preceding the changes.But that may not stop the athlete from assuming that there is a treatment effect, i.e.,\n",
    "attributing the improved performance to the changes he or she made.\n",
    "\n",
    "\n",
    "The Nobel prize-winning psychologist Daniel Kahneman tells a story about anIsraeli Air Force flight instructor who rejected Kahneman’s assertion that “rewards for improved performance work better than punishment for mistakes.” The instructor’s argument was “On many occasions I have praised flights cadets for clean execution of some aerobatic maneuver. The next time they try the same maneuver they usually do worse. On the other hand, I have often screamed into a cadet’s earphone for bad execution, and in general he does better on the next try.”153 It is natural for humans to imagine a treatment effect, because we like to think causally. But sometimes it is simply a matter of luck.\n",
    "\n",
    "Imagining a treatment effect when there is none can be dangerous. It can lead to the belief that vaccinations are harmful, that snake oil cures all aches and pains, or that investing exclusively in mutual funds that “beat the market” last year is a good strategy.\n",
    "\n",
    "![21.12](./img/21-12.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2213 Just Beware\n",
    "\n",
    "It would be easy, and fun, to fill a few hundred pages with a history of statistical abuses. \n",
    "\n",
    "But by now you probably got the message: \n",
    "\n",
    "<p style=\"font-size:150%;font-weight:700;color:blue;text-align:left\"> It’s just as easy to lie with numbers as it is to lie with words<p>\n",
    "\n",
    "Make sure that you understand what is actually being measured and how those “statistically significant” results were\n",
    "computed before you jump to conclusions.\n",
    "\n",
    "As Darrell Huff said, \n",
    "\n",
    "* **“`If you torture the data long enough, it will confess to anything`.”**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
